#### Approach

1. The first step was the find out the initial and transition probabilities of all the characters in our training file. We used the bc.train file provided in part 1 to find these probabilities. The functions used to find the initial and transition probabilties are :

   1. Train(train_txt_fname) : line 36 - This function takes an input file name and returns dictionaries of initial and transition probabilities of all characters present in the training file name provided.
   2. Prepare_training_data(train_txt_name) : line 86 - This function takes an input file name and returns a list of words after removing POS tags (using the Remove_pos function) from the bc.train file and adding a blank space to the end of each word, as we need to compute transitions of characters to blank space also.
   3. Remove_pos (line) : line 97 - This function takes an line of the input file as input and returns a list  and removes POS from it and returns a list of words, including an extra space at the end of each word.
   4. Create_character_dict(words_train_data) : line 46 : This function takes the list of words generated by Prepare_training_data function and returns two dictionaries of initial and transition probabilties of each character. We have normalised the probabilities and handled missing transitions and initials in the same function.

2. The second step was to compute the emission probabilities. Here we compute the probability of observing in the 'train_letters' for each character in the 'test_letters' by comparing their pixels list of lists, which can be represented as P ( observed pixels of a test_letter | letter in the train_letters). This is computed in the Emission_probability(train_letters, test_letters) function.

   Emission_probability(train_letters, test_letters) : line 109 - This function takes train_letters and test_letters as inputs and returns the emission probability of each character in test_letters against each character in the train_letters based on how many pixels match and returns a numpy array of length (train_letters) x length (test_letters) dimension, containing emission probabilities. Since there is noise associated with pixels of test_letters, we computed the emission_probability using Bayes classsifier as emission_probability =  (noise^not_matched) * ((1-noise)^matched). 

   Initially, we gave the same points to matched and not_matched pixels. This seemed to work fine and produce good results for easy files like 'test-0-0.png', however, tougher files like 'test-15-0.png' mostly provided us blank spaces in the output, which gave us the idea of giving more points to matched pixel with '*' and lesser points to matched blanks, as the active pixels help us identiy a character while the blank ones don't seem to be much significant. Similarly, giving more points to blank not matched pixels over matched pixels which are not blanks. Experiementing with different scores for matched, not_matched and noise values, we reached a conclusion of using noise=0.28 and scoring twice as much to matched active pixels as compared to blank ones. As different combinations provided better results for different images, we could not find a single best combination that would work for all the images.

3. The final step was to figure out the text's from the images using simplied and hmm. To compute these :

   1. Simplified (train_letters, test_letters) : line 127 - This function takes train_letters and test_letters as input and returns a string of predicted sentence as output. As we need to compute only the emission probability in the simplified model, we could find the maximum probability of each character in train_letters for given individual test_letters which results to the predicted sentence returned as output by Simplified(train_letters, test_letters) functions.

   2. Hmm_viterbi(train_letters, test_letters, character_initial, character_transition) : line 138 - In the hmm viterbi, the probability of finding each letter also depends on previous predicted character. Hence, the probablity of predicting each character is computed as 

      character_p = Max(emission_p * transition ( current character | previous predicted character))

 [This article](https://www.mygreatlearning.com/blog/pos-tagging/) which we used for part 1 proved an insightful resource in developing the code and intuition for part 3.

Following are the few outputs generated by the code, <br>

Output 1:<br>
Simple: 1t is so ordered.<br>
   HMM: 1t is so ordered.<br>
<br>
Output 2:<br>
Simple: Nos. 14-556. Argued Apri1 28, 2015 - Decided June 26, 2015<br>
   HMM: Nos. 14-556. Argued Apri1 28, 2015 - Decided June 26, 2015<br>
<br>
Output 3:<br>
Simple: Their p1ea is that they do respect it, respect it so deep1y that<br>
   HMM: Their p1ea is that they do respect it, respect it so deep1y that<br>